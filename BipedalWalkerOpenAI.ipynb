{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IA33lmvf1_92",
        "outputId": "8fbc95aa-1d12-457b-cce1-c56a41b06fff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Connecting to security.ubuntu.com (91.189.91\r                                                                                                    \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r                                                                                                    \r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers] [Waiting for headers]\r                                                                                                  \rHit:3 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "\r                                                                                                  \r0% [Connecting to archive.ubuntu.com (185.125.190.36)] [Waiting for headers]\r                                                                            \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Reading package lists... Done\n",
            "E: Unable to locate package python-opengl\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!apt-get update\n",
        "!apt-get -qq install python-opengl -y\n",
        "!apt-get -qq -y install xvfb ffmpeg\n",
        "!pip -q install pyvirtualdisplay\n",
        "!pip -q install piglet\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "from gym import wrappers"
      ],
      "metadata": {
        "id": "SVLXJ0bK2IDA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Hp():\n",
        "    # Hyperparameters\n",
        "    def __init__(self,\n",
        "                 episode_length=500,\n",
        "                 learning_rate=0.1,\n",
        "                 num_deltas=16,\n",
        "                 num_best_deltas=16,\n",
        "                 noise=0.03,\n",
        "                 seed=1,\n",
        "                 env_name='BipedalWalker-v2',\n",
        "                 record_every=10):\n",
        "        self.episode_length = episode_length\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_deltas = num_deltas\n",
        "        self.num_best_deltas = num_best_deltas\n",
        "        assert self.num_best_deltas <= self.num_deltas\n",
        "        self.noise = noise\n",
        "        self.seed = seed\n",
        "        self.env_name = env_name\n",
        "        self.record_every = record_every"
      ],
      "metadata": {
        "id": "-r9u1q3J2Vzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Normalizer():\n",
        "    # Normalizes the inputs\n",
        "    def __init__(self, nb_inputs):\n",
        "        self.n = np.zeros(nb_inputs)\n",
        "        self.mean = np.zeros(nb_inputs)\n",
        "        self.mean_diff = np.zeros(nb_inputs)\n",
        "        self.var = np.zeros(nb_inputs)\n",
        "\n",
        "    def observe(self, x):\n",
        "        self.n += 1.0\n",
        "        last_mean = self.mean.copy()\n",
        "        self.mean += (x - self.mean) / self.n\n",
        "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
        "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
        "\n",
        "    def normalize(self, inputs):\n",
        "        self.observe(inputs)\n",
        "        obs_mean = self.mean\n",
        "        obs_std = np.sqrt(self.var)\n",
        "        return (inputs - obs_mean) / obs_std"
      ],
      "metadata": {
        "id": "f_Xfae2n2Z51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ArsTrainer():\n",
        "    def __init__(self, env, input_size=None, output_size=None, hp=None, normalizer=None, monitor_dir=None):\n",
        "        self.env = env\n",
        "        self.input_size = input_size or self.env.observation_space.shape[0]\n",
        "        self.output_size = output_size or self.env.action_space.shape[0]\n",
        "        self.weights = np.zeros((self.output_size, self.input_size))\n",
        "        self.hp = hp or Hp()\n",
        "        self.normalizer = normalizer or Normalizer(self.input_size)\n",
        "        self.cur_step = 0;\n",
        "        self.set_monitor(monitor_dir)\n",
        "        self.record_video = False\n",
        "\n",
        "    def set_monitor(self, monitor_dir=None):\n",
        "        #use this method if you want to record the episode\n",
        "        #set the folder where the recorded video will be stored\n",
        "        if monitor_dir is not None:\n",
        "            should_record = lambda i: self.record_video\n",
        "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, force=True)\n",
        "            self.hp.episode_length = 2000\n",
        "\n",
        "    def learning_rate(self, decay=0.01):\n",
        "        return self.hp.learning_rate / (1 + decay * self.cur_step)\n",
        "\n",
        "    def train(self, n_steps):\n",
        "        for step in range(n_steps):\n",
        "            self.cur_step += 1\n",
        "            # initialize the random noise deltas and the positive/negative rewards\n",
        "            deltas = self.generate_deltas()\n",
        "            positive_rewards = np.zeros(self.hp.num_deltas)\n",
        "            negative_rewards = np.zeros(self.hp.num_deltas)\n",
        "            # play an episode each with positive deltas and negative deltas, collect rewards\n",
        "            for i in range(self.hp.num_deltas):\n",
        "                positive_rewards[i] = self.play_episode(self.weights + self.hp.noise * deltas[i])\n",
        "                negative_rewards[i] = self.play_episode(self.weights - self.hp.noise * deltas[i])\n",
        "\n",
        "            # Compute the standard deviation of all rewards\n",
        "            sigma_rewards = np.array(positive_rewards + negative_rewards).std()\n",
        "\n",
        "            # Sort the rollouts by the max(r_pos, r_neg) and select the deltas with best rewards\n",
        "            scores = {k: max(r_pos, r_neg) for k, (r_pos, r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
        "            order = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)[:self.hp.num_best_deltas]\n",
        "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
        "            # Update the policy\n",
        "            self.update_weights(rollouts, sigma_rewards)\n",
        "\n",
        "            # Only record video during evaluation, every n steps\n",
        "            if step % self.hp.record_every == 0:\n",
        "                self.record_video = True\n",
        "            # Play an episode with the new weights and print the score\n",
        "            reward_evaluation = self.play_episode(self.weights, train=False)\n",
        "            print('Step: ', step, 'Reward: ', reward_evaluation)\n",
        "            self.record_video = False\n",
        "\n",
        "    def update_weights(self, rollouts, sigma_rewards):\n",
        "        # sigma_rewards is the standard deviation of the rewards\n",
        "        step = np.zeros(self.weights.shape)\n",
        "        for r_pos, r_neg, delta in rollouts:\n",
        "            step += (r_pos - r_neg) * delta\n",
        "        self.weights += self.learning_rate() / (self.hp.num_best_deltas * sigma_rewards) * step\n",
        "\n",
        "    def play_episode(self, theta=None, train=True):\n",
        "        # play one episode of game\n",
        "        if theta is None:\n",
        "            theta = self.weights\n",
        "        obs = self.env.reset()\n",
        "        sum_reward = 0\n",
        "        episode = 0\n",
        "        while True:\n",
        "            episode += 1\n",
        "            # choose action using theta\n",
        "            action = self.predict(obs, theta)\n",
        "            obs, reward, done, _ = self.env.step(action)\n",
        "            sum_reward += reward\n",
        "            if done:\n",
        "                break\n",
        "            # break if reached max number of episodes\n",
        "            if episode >= self.hp.episode_length:\n",
        "                break\n",
        "\n",
        "        return sum_reward\n",
        "\n",
        "    def predict(self, inp, theta):\n",
        "        # predict action from input using theta\n",
        "        inp = self.normalizer.normalize(inp)\n",
        "        return theta @ inp\n",
        "\n",
        "    def generate_deltas(self):\n",
        "        return np.random.randn(self.hp.num_deltas, *self.weights.shape)\n"
      ],
      "metadata": {
        "id": "7ztwtm5e2gPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install swig\n",
        "!pip install gym[box2d]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B84ZfFlB3n6U",
        "outputId": "f206c973-6e00-489b-d321-9b3496071181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: swig in /usr/local/lib/python3.10/dist-packages (4.2.1)\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Using cached box2d-py-2.3.5.tar.gz (374 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Using cached pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.2.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376134 sha256=4a78bd8529cb307a04252eaecbeac153ae42c20f2356550cd41a7375c77b98a5\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make('BipedalWalker-v3')\n",
        "trainer = ArsTrainer(env)\n",
        "trainer.train(100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEg43pz32iv9",
        "outputId": "18507970-5269-4478-8893-65a0bf34d91d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step:  0 Reward:  -121.97701323504731\n",
            "Step:  1 Reward:  -15.583307119312188\n",
            "Step:  2 Reward:  -9.855134650859407\n",
            "Step:  3 Reward:  -2.2529806423482177\n",
            "Step:  4 Reward:  -21.14698464896552\n",
            "Step:  5 Reward:  -15.549281211480464\n",
            "Step:  6 Reward:  -2.1038549855274864\n",
            "Step:  7 Reward:  -11.919960764286046\n",
            "Step:  8 Reward:  -3.560869641966173\n",
            "Step:  9 Reward:  -9.979922836640032\n",
            "Step:  10 Reward:  -4.771576702069035\n",
            "Step:  11 Reward:  -7.39026327543413\n",
            "Step:  12 Reward:  -8.516660428660332\n",
            "Step:  13 Reward:  -3.8087913446375268\n",
            "Step:  14 Reward:  -5.717333625567242\n",
            "Step:  15 Reward:  -3.0273777369185915\n",
            "Step:  16 Reward:  -6.9932600975951225\n",
            "Step:  17 Reward:  -5.850128786782016\n",
            "Step:  18 Reward:  -5.764709460988472\n",
            "Step:  19 Reward:  -3.3343986124737977\n",
            "Step:  20 Reward:  -3.77886123191379\n",
            "Step:  21 Reward:  -6.966957774605357\n",
            "Step:  22 Reward:  -2.8720948613111283\n",
            "Step:  23 Reward:  -3.9166996251580746\n",
            "Step:  24 Reward:  -97.7530494970671\n",
            "Step:  25 Reward:  -1.3025401946140018\n",
            "Step:  26 Reward:  -5.39222359359319\n",
            "Step:  27 Reward:  -6.244665408492858\n",
            "Step:  28 Reward:  -4.009303457731493\n",
            "Step:  29 Reward:  -2.983638848028346\n",
            "Step:  30 Reward:  -2.943899628282096\n",
            "Step:  31 Reward:  -2.5333462249620253\n",
            "Step:  32 Reward:  -2.426728468295686\n",
            "Step:  33 Reward:  -3.585825964447097\n",
            "Step:  34 Reward:  -2.774133623059283\n",
            "Step:  35 Reward:  -0.5781164321036509\n",
            "Step:  36 Reward:  2.648917578791977\n",
            "Step:  37 Reward:  -0.7019212070084485\n",
            "Step:  38 Reward:  -1.4882106546737424\n",
            "Step:  39 Reward:  1.8879170613733611\n",
            "Step:  40 Reward:  -3.860799107516168\n",
            "Step:  41 Reward:  -4.901533748873057\n",
            "Step:  42 Reward:  -6.591772438755628\n",
            "Step:  43 Reward:  1.429452914703521\n",
            "Step:  44 Reward:  -3.5538729393032438\n",
            "Step:  45 Reward:  -5.749835644863743\n",
            "Step:  46 Reward:  -1.86965650342921\n",
            "Step:  47 Reward:  -0.5318864714434435\n",
            "Step:  48 Reward:  -2.05914575395448\n",
            "Step:  49 Reward:  1.9574267827223888\n",
            "Step:  50 Reward:  -2.135434246995157\n",
            "Step:  51 Reward:  1.234034872181099\n",
            "Step:  52 Reward:  -0.5190005391979893\n",
            "Step:  53 Reward:  0.9893054234312212\n",
            "Step:  54 Reward:  -0.15322009273365206\n",
            "Step:  55 Reward:  0.09903737104764078\n",
            "Step:  56 Reward:  1.0260534981761291\n",
            "Step:  57 Reward:  2.917934176530902\n",
            "Step:  58 Reward:  5.076732817195789\n",
            "Step:  59 Reward:  5.985701488883848\n",
            "Step:  60 Reward:  5.7495000588413125\n",
            "Step:  61 Reward:  5.370588138156539\n",
            "Step:  62 Reward:  4.418970741696045\n",
            "Step:  63 Reward:  5.699082832402942\n",
            "Step:  64 Reward:  4.070041757470947\n",
            "Step:  65 Reward:  5.21511989114997\n",
            "Step:  66 Reward:  6.017164373632688\n",
            "Step:  67 Reward:  1.9742725577581262\n",
            "Step:  68 Reward:  5.4029041737335275\n",
            "Step:  69 Reward:  1.3222858505963635\n",
            "Step:  70 Reward:  4.193276863600325\n",
            "Step:  71 Reward:  1.0624594674408323\n",
            "Step:  72 Reward:  4.402492004554834\n",
            "Step:  73 Reward:  11.393500364099827\n",
            "Step:  74 Reward:  1.3113768324096924\n",
            "Step:  75 Reward:  4.32888252814259\n",
            "Step:  76 Reward:  5.191092385094514\n",
            "Step:  77 Reward:  4.307082693418824\n",
            "Step:  78 Reward:  7.179658037736361\n",
            "Step:  79 Reward:  1.5692228213035244\n",
            "Step:  80 Reward:  6.899951370386292\n",
            "Step:  81 Reward:  6.206502385619552\n",
            "Step:  82 Reward:  2.8693506428121553\n",
            "Step:  83 Reward:  6.407280282991757\n",
            "Step:  84 Reward:  9.440185915260242\n",
            "Step:  85 Reward:  6.015547732316393\n",
            "Step:  86 Reward:  6.934954746489328\n",
            "Step:  87 Reward:  7.71197767513858\n",
            "Step:  88 Reward:  3.735544673655903\n",
            "Step:  89 Reward:  5.665899735531478\n",
            "Step:  90 Reward:  5.39256230937892\n",
            "Step:  91 Reward:  7.3705684462200844\n",
            "Step:  92 Reward:  7.015801349081283\n",
            "Step:  93 Reward:  6.898425762667625\n",
            "Step:  94 Reward:  5.782884321108443\n",
            "Step:  95 Reward:  6.022306319104741\n",
            "Step:  96 Reward:  3.1472002213452197\n",
            "Step:  97 Reward:  5.494825298659431\n",
            "Step:  98 Reward:  4.238032893470296\n",
            "Step:  99 Reward:  9.781173432743119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "DIR_PATH = '/content/drive/MyDrive/Colab Notebooks/BipedalWalkervids'"
      ],
      "metadata": {
        "id": "TEIfH-f2KrRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "def mkdir(base, name):\n",
        "    path = os.path.join(base, name)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    return path"
      ],
      "metadata": {
        "id": "ZjhMTufqK0vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "videos_dir = mkdir(DIR_PATH, 'videos')\n",
        "monitor_dir = mkdir(videos_dir, 'bi')"
      ],
      "metadata": {
        "id": "cfcarRfMK3i-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start virtual display\n",
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1024, 768))\n",
        "display.start()\n",
        "import os\n",
        "# os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n"
      ],
      "metadata": {
        "id": "rzglE_T-K6p_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.set_monitor(monitor_dir)"
      ],
      "metadata": {
        "id": "zgyVdEw-21Od",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 335
        },
        "outputId": "50f7375d-f1d4-4a6e-dd0c-0c1c69cf60d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'gym.wrappers' has no attribute 'Monitor'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-244789e2bfb2>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_monitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-2b556378d6c3>\u001b[0m in \u001b[0;36mset_monitor\u001b[0;34m(self, monitor_dir)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmonitor_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mshould_record\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmonitor_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvideo_callable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshould_record\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'gym.wrappers' has no attribute 'Monitor'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "trainer.train(100)"
      ],
      "metadata": {
        "id": "PkD5bNGt228M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}